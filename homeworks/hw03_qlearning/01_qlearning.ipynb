{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJpPNqDMRTip"
      },
      "source": [
        "## HW 3: Q-learning\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Q-learning and apply it to RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4FAZHkowtYQZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DkIW11kMtYQa",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 1: Tabular Q-learning with SoftMax policy\n",
        "\n",
        "We want you to implement Q-learning algorithm with softmax policy.\n",
        "\n",
        "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below.\n",
        "\n",
        "SoftMax policy means that actions probabilities are computed from q-values using softmax function, where $\\tau$ is the softmax temperature:\n",
        "\n",
        "$$ \\pi(a_i \\mid s) = \\operatorname{softmax} \\left( \\left\\{ {Q(s, a_j) \\over \\tau} \\right\\}_{j=1}^n \\right)_i = {\\operatorname{exp} \\left( Q(s,a_i) / \\tau \\right)  \\over {\\sum_{j}  \\operatorname{exp} \\left( Q(s,a_j) / \\tau  \\right)}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrNzay5kT4cM"
      },
      "source": [
        "First, implement softmax. Do not forget that exponent of large value might be too big to fit in float. Substract maximum for numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DQCiD1jl1S0R"
      },
      "outputs": [],
      "source": [
        "def my_softmax(values: np.ndarray, T=1.):\n",
        "    x = values / T\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    probas = e_x / np.sum(e_x)\n",
        "    assert probas is not None\n",
        "    return probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2y5b61jUagq"
      },
      "source": [
        "Now check your Softmax using the following tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDIvjlg6UXt-",
        "outputId": "7529ce55-c417-46d0-dc2b-bb9db9829973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.24816138e-18 9.35719813e-14 2.06106005e-09 4.53978686e-05\n",
            " 9.99954600e-01]\n",
            "Passed for temp=0.1\n",
            "[2.90075868e-04 2.14338686e-03 1.58376057e-02 1.17024957e-01\n",
            " 8.64703974e-01]\n",
            "Passed for temp=0.5\n",
            "[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n",
            "Passed for temp=1\n",
            "[0.12885125 0.15737927 0.19222347 0.23478228 0.28676373]\n",
            "Passed for temp=5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "test_data = np.arange(5)\n",
        "for temp in [0.1, 0.5, 1, 5]:\n",
        "  local_softmax = my_softmax(test_data, T=temp)\n",
        "  with torch.no_grad():\n",
        "    torch_softmax = torch.softmax(torch.from_numpy(test_data)/temp, dim=-1)\n",
        "  assert np.allclose(local_softmax, torch_softmax.numpy())\n",
        "  print(f'Passed for temp={temp}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cheRa9OtVhrg"
      },
      "source": [
        "Simple visualization for your convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "R4WD6jCeVg7r",
        "outputId": "6457d45c-d179-4e00-a716-94a2776b6db6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1630/4133860864.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Example softmax with temp = {temp}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "temp = 1\n",
        "plt.bar(test_data, my_softmax(test_data, T=temp))\n",
        "plt.title(f'Example softmax with temp = {temp}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19MpPxBKT5xI"
      },
      "source": [
        "Now implement the `QLearningAgent`. You will solve several environments using it and later submit to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ar8R55tYQc",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, discount, get_legal_actions, temp=1.):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
        "        Instance variables you have access to\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "          - self.temp (softmax temperature)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly.\n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.discount = discount\n",
        "        self.temp = temp\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\"Returns Q(state,action)\"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\"Sets the Qvalue for [state,action] to the given value\"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Calculate the approximation of value function V(s).\n",
        "        value = max(self.get_qvalue(state, action) for action in possible_actions)\n",
        "        assert value is not None\n",
        "\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Calculate the updated value of Q(s, a).\n",
        "        qvalue = (1 - self.alpha) * self.get_qvalue(state, action) + self.alpha * (reward + self.discount * self.get_value(next_state))\n",
        "        assert qvalue is not None\n",
        "\n",
        "        self.set_qvalue(state, action, qvalue)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values).\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Choose the best action wrt the qvalues.\n",
        "\n",
        "        best_action = max(possible_actions, key=lambda action: self.get_qvalue(state, action))\n",
        "\n",
        "        assert best_action is not None\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_softmax_policy(self, state):\n",
        "        \"\"\"\n",
        "        Compute all actions probabilities in the current state according\n",
        "        to their q-values using softmax policy.\n",
        "\n",
        "        Actions probability should be computed as\n",
        "        p(a_i|s) = softmax([q(s, a_1), q(s, a_2), ... q(s, a_k)])_i\n",
        "        Softmax temperature is set to `self.temp`.\n",
        "        See the formula in the notebook for more details\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Compute all actions probabilities in the current state using softmax\n",
        "        q_values = np.array([self.get_qvalue(state, action) for action in possible_actions])\n",
        "        assert q_values is not None\n",
        "        probabilities = my_softmax(q_values, self.temp)\n",
        "        assert probabilities is not None\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.\n",
        "        Select actions according to softmax policy.\n",
        "\n",
        "        Note: To pick randomly from a list, use np.random.choice(..., p=actions_probabilities)\n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Select the action to take in the current state according to the policy\n",
        "        actions_probabilities = self.get_softmax_policy(state)\n",
        "        chosen_action = np.random.choice(possible_actions, p=actions_probabilities)\n",
        "        assert chosen_action is not None\n",
        "        return chosen_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OeztZlsFtYQd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Try it on taxi\n",
        "\n",
        "Here we use the Q-Learning agent on the Taxi-v3 environment from OpenAI gym.\n",
        "You will need to complete a few of its functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vhB2v6tYQd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g95SWypZtYQd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "s, _ = env.reset()\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YlQCuGWtYQe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "agent = QLearningAgent(alpha=0.5, discount=0.99, get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI3QI_FUtYQe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should\n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s.\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _, _ = env.step(a)\n",
        "\n",
        "        # train (update) agent for state s\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBJIrxdQ2Oi_"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        plt.title(\"mean reward = {:.1f}\".format(np.mean(rewards[-10:])))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JHTHvi_btYQf",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Step 2: Discretized state spaces.\n",
        "\n",
        "Let's try solving the same `CartPole-v1` problem using Tabular Q-learning.\n",
        "\n",
        "This environment has a continuous set of possible states, so we will have to group them into bins somehow.\n",
        "\n",
        "Simple binarization is already present. You can play with to see what happens.\n",
        "\n",
        "The simplest way is to use `round(x, n_digits)` (or `np.round`) to round a real number to a given amount of digits. The tricky part is to get the `n_digits` right for each state to train effectively.\n",
        "\n",
        "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNFlU66XtYQf",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    return gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env = make_env()\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"first state: %s\" % (env.reset()[0]))\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ulL66TiDtYQf",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Play a few games\n",
        "\n",
        "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZAjeGB5tYQf",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def visualize_cartpole_observation_distribution(seen_observations):\n",
        "    seen_observations = np.array(seen_observations)\n",
        "\n",
        "    # The meaning of the observations is documented in\n",
        "    # https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
        "\n",
        "    f, axarr = plt.subplots(2, 2, figsize=(16, 9), sharey=True)\n",
        "    for i, title in enumerate([\"Cart Position\", \"Cart Velocity\", \"Pole Angle\", \"Pole Velocity At Tip\"]):\n",
        "        ax = axarr[i // 2, i % 2]\n",
        "        ax.hist(seen_observations[:, i], bins=20)\n",
        "        ax.set_title(title)\n",
        "        xmin, xmax = ax.get_xlim()\n",
        "        ax.set_xlim(min(xmin, -xmax), max(-xmin, xmax))\n",
        "        ax.grid()\n",
        "    f.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kaa7AzyqtYQg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "seen_observations = []\n",
        "for _ in range(1000):\n",
        "    s, _ = env.reset()\n",
        "    seen_observations.append(s)\n",
        "    done = False\n",
        "    while not done:\n",
        "        s, r, done, _, _ = env.step(env.action_space.sample())\n",
        "        seen_observations.append(s)\n",
        "\n",
        "visualize_cartpole_observation_distribution(seen_observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hxvoaLCPtYQg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Discretize environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzgdnHPJtYQg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from gymnasium.core import ObservationWrapper\n",
        "\n",
        "\n",
        "class Discretizer(ObservationWrapper):\n",
        "    def observation(self, state):\n",
        "        # Hint: you can do that with round(x, n_digits).\n",
        "        # You may pick a different n_digits for each dimension.\n",
        "\n",
        "        features_ndigits = [1, 1, 1, 1]\n",
        "        for feature_index in range(len(state)):\n",
        "            state[feature_index] = round(state[feature_index], features_ndigits[feature_index])\n",
        "\n",
        "        return tuple(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJI-8eddtYQg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "env = Discretizer(make_env())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg7YI6HMtYQh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "seen_observations = []\n",
        "for _ in range(1000):\n",
        "    s, _ = env.reset()\n",
        "    seen_observations.append(s)\n",
        "    done = False\n",
        "    while not done:\n",
        "        s, r, done, _, _ = env.step(env.action_space.sample())\n",
        "        seen_observations.append(s)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "visualize_cartpole_observation_distribution(seen_observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kelG9gDYtYQh",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Learn discretized policy\n",
        "\n",
        "Now let's train a policy that uses discretized state space.\n",
        "\n",
        "__Tips:__\n",
        "\n",
        "* Note that increasing the number of digits for one dimension of the observations increases your state space by a factor of $10$.\n",
        "* If your discretization is too fine-grained, your agent will take much longer than 10000 steps to converge. You can either increase the number of iterations and reduce softmax temperature decay or change discretization. In practice we found that this kind of mistake is rather frequent.\n",
        "* If your discretization is too coarse, your agent may fail to find the optimal policy. In practice we found that on this particular environment this kind of mistake is rare.\n",
        "* **Start with a coarse discretization** and make it more fine-grained if that seems necessary.\n",
        "* Having $10^3$â€“$10^4$ distinct states is recommended (`len(agent._qvalues)`), but not required.\n",
        "* If things don't work without annealing $\\varepsilon$, consider adding that, but make sure that it doesn't go to zero too quickly.\n",
        "\n",
        "A reasonable agent should attain an average reward of at least 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngVLYpgZtYQh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({\"x\": np.asarray(x)}).x.ewm(span=span).mean().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deDOLOCHtYQh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "agent = QLearningAgent(alpha=0.5, discount=0.99, get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2wXZ32o0T2G"
      },
      "outputs": [],
      "source": [
        "rewards = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyD96lJLtYQi",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(5000):\n",
        "    reward = play_and_train(env, agent)\n",
        "    rewards.append(reward)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        rewards_ewma = moving_average(rewards)\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.plot(rewards, label=\"rewards\")\n",
        "        plt.plot(rewards_ewma, label=\"rewards ewma@100\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.title(\"rewards ewma@100 = {:.1f}\".format(rewards_ewma[-1]))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgO_THYStYQi",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Your agent has learned {} Q-values.\".format(len(agent._qvalues)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wTJa3D0DtYQi",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 3: EV-SARSA with softmax policy\n",
        "\n",
        "The policy we're going to use is still softmax, but now the state  value is computed differently.\n",
        "\n",
        "We inherit from the existing implementation of `QLearningAgent` and only update the `get_value` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9Bkk5QutYQm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class EVSarsaAgent(QLearningAgent):\n",
        "    \"\"\"\n",
        "    An agent that changes some of q-learning functions to implement Expected Value SARSA.\n",
        "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
        "    If it doesn't, please add\n",
        "        def update(self, state, action, reward, next_state):\n",
        "            and implement it for Expected Value SARSA's V(s')\n",
        "    \"\"\"\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Returns Vpi for current state under the softmax policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Compute the value of the current state under the softmax policy.\n",
        "        value = None\n",
        "        assert value is not None\n",
        "\n",
        "        return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "j4d4mdFftYQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Cliff World\n",
        "\n",
        "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n",
        "\n",
        "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/cliffworld.png width=600>\n",
        "<center><i>Image from CS188</i></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfIWhm8stYQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CliffWalking-v1\", render_mode=\"rgb_array\")\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiAAsXj1tYQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Our cliffworld has one difference from what's in the image: there is no wall.\n",
        "# Agent can choose to go as close to the cliff as it wishes.\n",
        "# x:start, T:exit, C:cliff, o: flat ground\n",
        "\n",
        "env.reset()\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzUztYIntYQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "agent_sarsa = EVSarsaAgent(alpha=0.25, discount=0.99, get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, discount=0.99, get_legal_actions=lambda s: range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTiW9yP2tYQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({\"x\": np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(5000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"EVSARSA mean reward =\", np.mean(rewards_sarsa[-100:]))\n",
        "        print(\"QLEARNING mean reward =\", np.mean(rewards_ql[-100:]))\n",
        "        plt.plot(moving_average(rewards_sarsa), label=\"ev_sarsa\")\n",
        "        plt.plot(moving_average(rewards_ql), label=\"qlearning\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "83t7Z48otYQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's now see what did the algorithms learn by visualizing their actions at every state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u9oF-bKtYQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def draw_policy(agent):\n",
        "    \"\"\"Prints CliffWalkingEnv policy with arrows. Hard-coded.\"\"\"\n",
        "\n",
        "    env = gym.make(\"CliffWalking-v1\", render_mode=\"ansi\")\n",
        "    env.reset()\n",
        "    grid = [x.split(\"  \") for x in env.render().split(\"\\n\")[:4]]\n",
        "\n",
        "    n_rows, n_cols = 4, 12\n",
        "    start_state_index = 36\n",
        "    actions = \"^>v<\"\n",
        "\n",
        "    for yi in range(n_rows):\n",
        "        for xi in range(n_cols):\n",
        "            if grid[yi][xi] == \"C\":\n",
        "                print(\" C \", end=\"\")\n",
        "            elif (yi * n_cols + xi) == start_state_index:\n",
        "                print(\" X \", end=\"\")\n",
        "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
        "                print(\" T \", end=\"\")\n",
        "            else:\n",
        "                print(\" %s \" % actions[agent.get_best_action(yi * n_cols + xi)], end=\"\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QidUsod2tYQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(agent_sarsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgDhSZopXEWg"
      },
      "source": [
        "Congratulations! Finally, copy the `QLearningAgent`, `EVSarsaAgent` and `my_softmax` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}